# -*- coding: utf-8 -*-
"""NLI-TR / Hypothesis only baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vuKlI8Q5ZlDnf2fhFs_-WnjUhuuyUuCp

# NLI-TR / Hypothesis only baseline

In this experimentation, we want to show the use of NLI-TR dataset on a particular use case where we would like to answer the main question: 

> *What is the difference between hypothesis-only baseline and the actual models when we fine-tune off-the-shelf models on NLI-TR.*

Thanks **Lasha Abzianidze** for this insightful question in our Gather.town session at EMNLP 2020! 

*Disclaimer*: The code is mostly based on the examples in the following repositories and the documentation of Huggingface Datasets and Transformers.

*   https://github.com/huggingface/transformers
*   https://github.com/huggingface/datasets
*   https://github.com/cgpotts/cs224u

## Setup
"""

import transformers

"""## Dataset readers"""

import torch
from datasets import load_dataset

class NLITRReader(torch.utils.data.Dataset):
  def __init__(self, dataset_name, split_name, max_example_num=-1):
    data_files_all = {"train": "data_train.json", "test": "data_test.json", "validation": "data_dev.json"}
    self.dataset = load_dataset('json', data_files=data_files_all)# data_files='data_train.json' #('nli_tr', dataset_name) 
    self.split_name = split_name
    self.max_example_num = max_example_num

    #train_dataset, validation_dataset= self.dataset[''].train_test_split(test_size=0.1, n_samples=len(self.dataset)).values()
    #self.dataset = DatasetDict({'train': train_dataset, 'validation': validation_dataset})

  def read(self):
      count = 0
      for example in self.dataset[self.split_name]:
          print(self.dataset)

          if example['gold_label'] == -1: # skip examples having no gold value.
              continue
          count += 1
          if self.max_example_num > 0 and count >= self.max_example_num:
             break
          yield example

import torch
class NLITRDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

"""## Trainer"""

import torch
import pandas as pd
from transformers import TrainingArguments, Trainer, AutoConfig, AutoTokenizer, AutoModelForSequenceClassification

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
    }

MAX_TRAIN_EXAMPLE_NUM = -1 #2048
MAX_EVALUATION_EXAMPLE_NUM = -1 #512
class NLITRTrainer():
    def __init__(self, 
                 model_name='bert-base-cased', 
                 dataset_name='data', ###
                 evaluation_split='validation',
                 num_labels=3, 
                 hypothesis_only=False):
        self.model_name = model_name
        self.dataset_name = dataset_name
        self.evaluation_split = evaluation_split
        self.hypothesis_only = hypothesis_only
        self.max_train_example_num = MAX_TRAIN_EXAMPLE_NUM
        self.max_evaluation_example_num = MAX_EVALUATION_EXAMPLE_NUM

        print('You can set the values of the following parameters via the global variables MAX_TRAIN_EXAMPLE_NUM and MAX_EVALUATION_EXAMPLE_NUM (-1 to use all examples in the splits)')
        print('max_train_example_num',self.max_train_example_num)
        print('max_evaluation_example_num',self.max_evaluation_example_num)
        self.prepare_for_training()
    
    def prepare_for_training(self):
        self.prepare_model()
        self.prepare_datasets()
        self.prepare_trainer()

    def prepare_model(self):
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.config = AutoConfig.from_pretrained(self.model_name, num_labels=3)
        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.config)
        
    def get_dataset(self, split_name, max_example_num):
        df = pd.DataFrame(list(NLITRReader(dataset_name=self.dataset_name, split_name=split_name, max_example_num=max_example_num).read()))
        labels = df['gold_label'].values.tolist() #######


        for i, label in enumerate(labels):
            if label == "contradiction":
                labels[i] = 2
            elif label == "neutral":
                labels[i] = 1
            elif label == "entailment":
                labels[i] = 0

        premises = df['sentence1'].values.tolist() ######
        if self.hypothesis_only:
            input = self.tokenizer(premises, truncation=True, padding=True)
        else:
            hypotheses = df['sentence2'].values.tolist()
            input = self.tokenizer(premises, hypotheses, truncation=True, padding=True)
        
        dataset = NLITRDataset(input, labels)
        return dataset

    def prepare_datasets(self):
        self.train_dataset = self.get_dataset('train', max_example_num=self.max_train_example_num)
        self.evaluation_dataset = self.get_dataset(self.evaluation_split, max_example_num=self.max_evaluation_example_num)
      
    def prepare_trainer(self):
        training_args = TrainingArguments(
            output_dir='./results',          # output directory
            num_train_epochs=1,              # total number of training epochs
            per_device_train_batch_size=4,   # batch size per device during training
            per_device_eval_batch_size=4,   # batch size for evaluation
            gradient_accumulation_steps=32,  # gradient accumulation steps to increase effective batch size on GPU.
            warmup_steps=500,                # number of warmup steps for learning rate scheduler
            weight_decay=0.01,               # strength of weight decay
            logging_dir='./logs',            # directory for storing logs
            logging_steps=10
        )

        self.trainer = Trainer(
            model=self.model,                         # the instantiated ðŸ¤— Transformers model to be trained
            args=training_args,                       # training arguments, defined above
            train_dataset=self.train_dataset,         # training dataset
            eval_dataset=self.evaluation_dataset,     # evaluation dataset,
            compute_metrics=compute_metrics
        )

    def train(self):
        train_results = self.trainer.train()
        return train_results

    def predict(self):
        pred_results = self.trainer.prediction_step()
        return pred_results

    def evaluate(self):
        eval_results = self.trainer.evaluate()
        return eval_results

"""## Experiment Manager

This is a simple experiment manager that runs a series of experiments with the given set of hyperparameters and returns the resulting metrics.
"""

import copy
import numpy as np
import random

class NLIExperiment:
    def __init__(self, experiment_parameters, seed=1234):
        self.experiment_parameters = experiment_parameters
        self.set_random_seed(seed)
    
    def set_random_seed(self, seed):
        np.random.seed(seed)
        random.seed(seed)
        torch.manual_seed(seed)
    
    def run(self):
        experiment_results = []
        
        for model_name in self.experiment_parameters['model_names']:
            experiment_parameters = {}
            experiment_parameters['model_name'] = model_name

            for dataset_name, evaliation_split_names in self.experiment_parameters['dataset_info'].items():
                experiment_parameters['dataset_name'] = dataset_name
            
                for evaliation_split_name in evaliation_split_names:
                    experiment_parameters['evaliation_split_name'] = evaliation_split_name

                    for param_key, param_values in self.experiment_parameters['params'].items():
                        print("param::::::::\n", self.experiment_parameters['params'].items())
                        for param_value in param_values:
                            experiment_parameters[param_key] = param_value
                            print('\n\nA new experiment started...')
                            nlitr_trainer = NLITRTrainer(model_name=model_name, dataset_name=dataset_name, evaluation_split=evaliation_split_name, **{param_key:param_value})

                            print('Training...')
                            train_results = nlitr_trainer.train()
                            print('Evaluating...')
                            eval_results = nlitr_trainer.evaluate()
                             
                            experiment_parameters.update(eval_results)
                            print('\nexperiment parameters:', experiment_parameters)
                            print('experiment results:', eval_results)
                            experiment_results.append(copy.deepcopy(experiment_parameters))
        return experiment_results

## Experiments

experiment_parameters0 = {
     'model_names' : ['dbmdz/bert-base-turkish-cased'], #alternative values: 'model_names' : ['bert-base-cased', 'bert-base-multilingual-cased', 'dbmdz/bert-base-turkish-cased'] 
     'dataset_info' : {'snli_tr': ['validation'], 'multinli_tr': ['validation_matched']},   #alternative values: {'snli_tr': ['validation', 'test'], 'multinli_tr': ['validation_matched', 'validation_mismatched']}
     'params' : {'hypothesis_only': [True, False]}
}


experiment_parameters = {
     'model_names' : ['bert-base-cased', 'bert-base-multilingual-cased', 'dbmdz/bert-base-turkish-cased'], #alternative values: 'model_names' : ['bert-base-cased', 'bert-base-multilingual-cased', 'dbmdz/bert-base-turkish-cased'] 
     'dataset_info' : {'data': ['test', 'validation'] },   #alternative values: {'snli_tr': ['validation', 'test'], 'multinli_tr': ['validation_matched', 'validation_mismatched']}
     'params' : {'hypothesis_only': [True, False]}
}

# 'dataset_info' : {'data_train.json': ['train'], 'data_test.json': ['test'], 'data_dev.json':['validation']}, 


# 
# #You may set different values for the size of training and evaluation splits for fast iterations (-1 to use all examples in the splits). 
# MAX_TRAIN_EXAMPLE_NUM = 2048
# MAX_EVALUATION_EXAMPLE_NUM = 512


experiment = NLIExperiment(experiment_parameters)
experiment_result = experiment.run()


experiment_result_df = pd.DataFrame(experiment_result)
experiment_result_df.head(n=100) #show all dataframe

print(experiment_result_df.head(n=100))